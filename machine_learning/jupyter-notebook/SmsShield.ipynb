{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f1f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9791318196371965\n",
      "AUC Score: 0.9756279327943588\n",
      "Message: Hey, how are you doing? Long time no see!\n",
      "Classified as: ham\n",
      "\n",
      "Message: You've won a free ticket to the Bahamas. Call now to claim!\n",
      "Classified as: spam\n",
      "\n",
      "Message: Let's catch up over coffee next week.\n",
      "Classified as: ham\n",
      "\n",
      "Message: Your account has been compromised. Please reset your password immediately.\n",
      "Classified as: ham\n",
      "\n",
      "Message: Don't forget to submit the report by Monday.\n",
      "Classified as: ham\n",
      "\n",
      "Message: Wie geht es dir? Lange nicht mehr gesehen!\n",
      "Classified as: ham\n",
      "\n",
      "Message: Sie haben einen kostenlosen Flug auf die Bahamas gewonnen. Rufen Sie jetzt an, um Ihren Gewinn zu beanspruchen!\n",
      "Classified as: spam\n",
      "\n",
      "Message: Lass uns nächste Woche auf einen Kaffee treffen.\n",
      "Classified as: ham\n",
      "\n",
      "Message: Ihr Konto wurde kompromittiert. Bitte setzen Sie sofort Ihr Passwort zurück.\n",
      "Classified as: ham\n",
      "\n",
      "Message: Vergessen Sie nicht, den Bericht bis Montag einzureichen.\n",
      "Classified as: ham\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define file paths\n",
    "path_english_multilingual = '/home/philipp/AndroidStudioProjects/AntiSMSScam/dataset/data-en-hi-de-fr.csv'\n",
    "path_spam_ham = '/home/philipp/AndroidStudioProjects/AntiSMSScam/dataset/spam-ham-dataset.csv'\n",
    "\n",
    "# Load datasets\n",
    "english_multilingual_data = pd.read_csv(path_english_multilingual)\n",
    "spam_ham_data = pd.read_csv(path_spam_ham, encoding='ISO-8859-1')\n",
    "\n",
    "# Preprocess the first dataset (data-en-hi-de-fr.csv)\n",
    "english_data = english_multilingual_data[['labels', 'text']]\n",
    "german_data = english_multilingual_data[['labels', 'text_de']]\n",
    "german_data.columns = ['labels', 'text']  # Rename for consistency\n",
    "\n",
    "# Concatenate English and German data\n",
    "english_multilingual_data = pd.concat([english_data, german_data])\n",
    "\n",
    "# Preprocess the second dataset (spam-ham-dataset.csv)\n",
    "spam_ham_data = spam_ham_data[['v1', 'v2']]\n",
    "spam_ham_data.columns = ['labels', 'text']\n",
    "\n",
    "# Combine datasets\n",
    "combined_data = pd.concat([english_multilingual_data, spam_ham_data])\n",
    "\n",
    "# Clean the combined dataset\n",
    "combined_data['labels'].fillna(combined_data['labels'].dropna().unique()[0], inplace=True)  # Fill NaN values\n",
    "combined_data = combined_data.dropna(subset=['labels'])  # Drop rows with NaN in labels\n",
    "\n",
    "# Remove duplicates\n",
    "combined_data = combined_data.drop_duplicates(subset='text')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "combined_data['labels'] = label_encoder.fit_transform(combined_data['labels'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = combined_data['text']\n",
    "y = combined_data['labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to features\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test_vec)\n",
    "y_pred_proba = model.predict_proba(X_test_vec)[:, 1]  # Probability of positive class\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use weighted average for multiclass\n",
    "auc = roc_auc_score(y_test, y_pred_proba)  # AUC for binary classification\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC Score: {auc}\")\n",
    "\n",
    "# Function to classify new messages\n",
    "def classify_messages(messages):\n",
    "    messages_vec = vectorizer.transform(messages)\n",
    "    predictions = model.predict(messages_vec)\n",
    "    prediction_labels = label_encoder.inverse_transform(predictions)\n",
    "    return list(zip(messages, prediction_labels))\n",
    "\n",
    "# Test the function with a list of 10 messages (5 English, 5 German)\n",
    "test_messages = [\n",
    "    \"Hey, how are you doing? Long time no see!\",\n",
    "    \"You've won a free ticket to the Bahamas. Call now to claim!\",\n",
    "    \"Let's catch up over coffee next week.\",\n",
    "    \"Your account has been compromised. Please reset your password immediately.\",\n",
    "    \"Don't forget to submit the report by Monday.\",\n",
    "    \"Wie geht es dir? Lange nicht mehr gesehen!\",\n",
    "    \"Sie haben einen kostenlosen Flug auf die Bahamas gewonnen. Rufen Sie jetzt an, um Ihren Gewinn zu beanspruchen!\",\n",
    "    \"Lass uns nächste Woche auf einen Kaffee treffen.\",\n",
    "    \"Ihr Konto wurde kompromittiert. Bitte setzen Sie sofort Ihr Passwort zurück.\",\n",
    "    \"Vergessen Sie nicht, den Bericht bis Montag einzureichen.\"\n",
    "]\n",
    "\n",
    "classified_messages = classify_messages(test_messages)\n",
    "for message, label in classified_messages:\n",
    "    print(f\"Message: {message}\\nClassified as: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b4806ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/philipp/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: onnx in ./anaconda3/lib/python3.8/site-packages (1.16.1)\n",
      "Requirement already satisfied: onnxmltools in ./anaconda3/lib/python3.8/site-packages (1.12.0)\n",
      "Collecting skl2onnx\n",
      "  Obtaining dependency information for skl2onnx from https://files.pythonhosted.org/packages/26/80/836824c62ff0923b4c3b8af8332170bdc3ccb469a220535b40405a93b4fb/skl2onnx-1.16.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading skl2onnx-1.16.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in ./anaconda3/lib/python3.8/site-packages (from onnx) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in ./anaconda3/lib/python3.8/site-packages (from onnx) (3.20.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in ./anaconda3/lib/python3.8/site-packages (from skl2onnx) (1.2.2)\n",
      "Requirement already satisfied: onnxconverter-common>=1.7.0 in ./anaconda3/lib/python3.8/site-packages (from skl2onnx) (1.14.0)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from onnxconverter-common>=1.7.0->skl2onnx) (23.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.19->skl2onnx) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.19->skl2onnx) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.19->skl2onnx) (3.1.0)\n",
      "Downloading skl2onnx-1.16.0-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.6.4 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: skl2onnx\n",
      "Successfully installed skl2onnx-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "MissingShapeCalculator",
     "evalue": "Unable to find a shape calculator for type '<class 'xgboost.sklearn.XGBClassifier'>'.\nIt usually means the pipeline being converted contains a\ntransformer or a predictor with no corresponding converter\nimplemented in sklearn-onnx. If the converted is implemented\nin another library, you need to register\nthe converted so that it can be used by sklearn-onnx (function\nupdate_registered_converter). If the model is not yet covered\nby sklearn-onnx, you may raise an issue to\nhttps://github.com/onnx/sklearn-onnx/issues\nto get the converter implemented or even contribute to the\nproject. If the model is a custom model, a new converter must\nbe implemented. Examples can be found in the gallery.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingShapeCalculator\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40205/50067013.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Convert the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save the ONNX model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/convert.py\u001b[0m in \u001b[0;36mconvert_sklearn\u001b[0;34m(model, name, initial_types, doc_string, target_opset, custom_conversion_functions, custom_shape_calculators, custom_parsers, options, intermediate, white_op, black_op, final_types, dtype, naming, model_optim, verbose)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[convert_sklearn] convert_topology\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     onnx_model = convert_topology(\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mtopology\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mconvert_topology\u001b[0;34m(topology, model_name, doc_string, target_opset, options, remove_identity, verbose)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[0;31m# Traverse the graph from roots to leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[0;31m# This loop could eventually be parallelized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m     \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_operators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m     \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_topological_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mconvert_operators\u001b[0;34m(self, container, verbose)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                         \u001b[0m_check_variable_out_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_shape_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36mcall_shape_calculator\u001b[0;34m(self, operator)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Shape2] call infer_types for %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m             \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_graph_status_for_traversing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/skl2onnx/common/_topology.py\u001b[0m in \u001b[0;36minfer_types\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# Invoke a core inference function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             raise MissingShapeCalculator(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 \"Unable to find a shape calculator for type '{}'.\".format(\n\u001b[1;32m    632\u001b[0m                     \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_operator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingShapeCalculator\u001b[0m: Unable to find a shape calculator for type '<class 'xgboost.sklearn.XGBClassifier'>'.\nIt usually means the pipeline being converted contains a\ntransformer or a predictor with no corresponding converter\nimplemented in sklearn-onnx. If the converted is implemented\nin another library, you need to register\nthe converted so that it can be used by sklearn-onnx (function\nupdate_registered_converter). If the model is not yet covered\nby sklearn-onnx, you may raise an issue to\nhttps://github.com/onnx/sklearn-onnx/issues\nto get the converter implemented or even contribute to the\nproject. If the model is a custom model, a new converter must\nbe implemented. Examples can be found in the gallery.\n"
     ]
    }
   ],
   "source": [
    "# !pip install onnx onnxmltools skl2onnx\n",
    "import onnxmltools\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Define the initial type\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train_vec.shape[1]]))]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "\n",
    "# Save the ONNX model\n",
    "with open(\"xgboost_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef6fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
